Below is the data for a paper and its corresponding LLM-generated classification. Your job is to determine if the classification accurately reflects the information in the paper's title, abstract, and keywords.

Instructions:
1. Read the paper content carefully.
2. Compare the automated classification against the paper content.
3. Determine if the classification is a faithful representation of the paper.
4. Respond ONLY with a JSON object containing two fields:
   - `verified`: `true` if the classification is largely correct, `false` if it contains significant errors or misrepresentations, `null` if there's not enough data for a decision, you are unsure or cannot determine the accuracy.
   - `estimated_score`: An integer between 0 and 10 representing a finer-grained score for how accurate the automated classification was compared to the actual paper data. 0 for completelly inaccurate, 10 for completely accurate, or any integer inbetween.

Example Response Format (only output the JSON):
{{
  "verified": true,
  "estimated_score": 7
}}

The plaintext below shows the requirements for the original classification you'll be verifying:

```plaintext
Given the data from the specific paper at the end, fill in the following YAML structure exactly and convert it to JSON. Do not add, remove or move any fields.
Only write 'true' or 'false' if the contents given (abstract, title, keywords, etc.) make it clear that it is the case. If unsure, fill the field with null:

research_area: null #broad area: electrical engineering, computer sciences, medical, finances, etc, can be inferred by journal or conference name as well as abstract contents.
is_offtopic: null #We are looking for PCB automated defect detection papers (be it implementations or surveys on this specific field). Set this field to true if paper seems unrelated to *implementations of automated defect detection on electronic printed circuit boards*. If the paper talks about anything else entirely, set as offtopic. If the paper talks about defect detection in other areas instead of electronics manufacturing, it's also offtopic. When offtopic, answer null for all fields following this one. Only set this to false if at least one feature from the 'features' list below (including "other") can be set to true.
is_survey: null #true for survey/review/etc., false for implementations, new research, etc.
is_through_hole: null #true for papers that specify PTH, THT, etc., through-hole component mounting.
is_smt: null #true for papers that specify surface-mount component mounting (SMD, SMT).
is_x_ray: null  #true for X-ray inspection, false for standard optical (visible light) inspection.
features:  # true, false, null for unknown/unclear. Identify the types of defect which are detected by the implementation(s) described in the paper.
    solder: null #for solder quality defects: too little solder, too much solder, short circuit, broken solder or any other soldering issues. 
    polarity: null #for components installed in the correct place, but with wrong orientation (inverted polarity, wrong pin 1 placement, etc).
    wrong_component: null #for components installed in the wrong location, might also detect components being installed where none should be.
    missing_component: null #for detection of empty places where some component has to be installed (e.g. empty pads that aren't supposed to stay empty).
    tracks: null	#any track error detection: open track, short circuit, spurious copper, mouse bite, wrong trace space/width, etc.
    cosmetic: null #cosmetic defects (any manufacturing defect that does not actually affect functionality: scratches, dirt, etc.);
    holes: null #for hole plating, drilling defects and any other PCB hole issues.
    other: null #"string with any other types of defect detection not specified above"
technique: # true, false, null for unknown/unclear. Identify all techniques used in an implementation, or all techniques reviewed in a survey:
    classic_computer_vision_based: null # for general pattern recognition techniques that do not leverage machine learning. Includes manually-programmed feature analysis. May or may not leverage optimization algorithms, like genetic, PSO, etc.
    machine_learning_based: null #for any technique that use classic machine learning like SVM, Random Forest, K-NN, LVQ, etc. Does not include deep learning like CNNs or Transformers.
    dl_cnn_based: null #for techniques using standard CNN classifiers: ResNet, EfficientNet, etc., not region-based/single-stage segmentation models.
    dl_rcnn_based: null #for region-based CNN detectors: R-CNNs, EfficientDet, most YOLO versions.
    dl_transformer_based: null #true for any Transformer (attention)-based models (e.g. ViT, DETR, etc.). Also true for mixed CNN+Transformer (e.g. YOLOv12, but not other pure-CNN YOLOs). 
    dl_other: null #for any remaining deep learning techniques that don't fit any of the categories above, like autoencoders;
    hybrid: null #for papers that describe techniques that are explicitly defined as some hybrid of classic and ML/DL techniques. true for Hybrid/Ensemble methods including ML+DL, classic+DL, but not including 'hybrids' of DL+DL (e.g. CNN+Transformer).
    model: "name"	#comma-separated list if multiple models are used (YOLO, ResNet, DETR, etc.), null if not ML, "in-house" if unnamed ML model is developed in the paper itself.
    available_dataset: null #true if authors explicitly mention they're providing related datasets for the public, false if there's no dataset usage (e.g. for techniques not depending on a dataset) or if the dataset used is not provided to the public.
```
Please notice that the null may also have been recorded as None. Both are correct and have the same meaning for the parser.

Now, here is the Paper Content (real data) for your task:

*Title:* {title}
*Abstract:* {abstract}
*Keywords:* {keywords}
*Authors:* {authors}
*Publication Year:* {year}
*Publication Type:* {type}
*Publication Name:* {journal}

Automated Classification to Verify (inferred by a language model):

research_area: {research_area}
is_offtopic: {is_offtopic}
is_survey: {is_survey}
is_through_hole: {is_through_hole}
is_smt: {is_smt}
is_x_ray: {is_x_ray}
features:
{features}
technique:
{technique}

Your response is not being read by a human, it goes directly to an automated parser. After thinking through the request in <think></think> tags, output only the result in JSON format in plaintext without any other tags like ```json or similar.